{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ReyURLrMKo"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJuh2e9praxr"
      },
      "source": [
        "class Linear_attention(nn.Module):\n",
        "    \"\"\"Heart of https://arxiv.org/pdf/2006.16236.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, eps=1e-6):\n",
        "\n",
        "        assert d_model % n_heads == 0, 'd_model must be a multiple of n_heads'\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = self.d_model // self.n_heads\n",
        "        self.eps = eps\n",
        "        self.w_qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "        # Trick to get model device. Stolen from:\n",
        "        # https://stackoverflow.com/questions/58926054/how-to-get-the-device-type-of-a-pytorch-module-conveniently\n",
        "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def dev(self):\n",
        "        return self.dummy_param.device\n",
        "\n",
        "    def get_mask1(self, l):\n",
        "        # l -> [batch_size]\n",
        "        mask = torch.arange(l.max(), device=self.dev())[None, :] < l[:, None]\n",
        "        # mask -> [batch_size, seq_len]\n",
        "        mask = torch.logical_not(mask[..., None, None])\n",
        "        # mask -> [batch_size, seq_len, 1, 1]\n",
        "        return mask\n",
        "\n",
        "    def get_mask(self, l):\n",
        "        # l -> [batch_size]\n",
        "        mask = torch.arange(l.max(), device=self.dev())[None, :] < l[:, None]\n",
        "        # mask -> [batch_size, seq_len]\n",
        "        mask = torch.logical_not(mask[:, None, :, None])\n",
        "        # mask -> [batch_size, 1, seq_len, 1]\n",
        "        return mask\n",
        "\n",
        "    def zero_out_padded1(self, x, mask):\n",
        "        # x -> [batch_size, n_heads, seq_len, d_head]\n",
        "        # mask -> [batch_size, 1, seq_len, 1]\n",
        "        x.masked_fill_(mask, 0)\n",
        "        return x\n",
        "\n",
        "    def zero_out_padded(self, x, mask):\n",
        "        # x -> [batch_size, seq_len, n_heads, d_head]\n",
        "        # mask -> [batch_size, seq_len, 1, 1]\n",
        "        x.masked_fill_(mask, 0)\n",
        "        return x\n",
        "\n",
        "    def split_heads1(self, x):\n",
        "        batch_size, seq_len = x.shape[:2]\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "        return x.view(\n",
        "            batch_size, seq_len, self.n_heads, self.d_head).permute(0, 2, 1, 3)\n",
        "        # result -> [batch_size, n_heads, seq_len, d_head]\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len = x.shape[:2]\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "        return x.view(batch_size, seq_len, self.n_heads, self.d_head)\n",
        "        # result -> [batch_size, seq_len, n_heads, d_head]\n",
        "\n",
        "    def join_heads1(self, x):\n",
        "        batch_size, seq_len = x.shape[0], x.shape[2]\n",
        "        # x -> [batch_size, n_heads, seq_len, d_head]\n",
        "        return x.permute(0, 2, 1, 3).view(batch_size, seq_len, self.d_model)\n",
        "        # result -> [batch_size, seq_len, d_model]\n",
        "\n",
        "    def join_heads(self, x):\n",
        "        batch_size, seq_len = x.shape[:2]\n",
        "        # x -> [batch_size, seq_len, n_heads, d_head]\n",
        "        return x.view(batch_size, seq_len, self.d_model)\n",
        "        # result -> [batch_size, seq_len, d_model]\n",
        "\n",
        "    def kernel1(self, x):\n",
        "        return F.elu(x) + 1\n",
        "\n",
        "    def linear_attention1(self, q, k, v):\n",
        "        # q, k, v -> [batch_size, n_heads, seq_len, d_head]\n",
        "        kv = torch.einsum('bnsd,bnsd->bndd', k, v)\n",
        "        # kv -> [batch_size, n_heads, d_head, d_head]\n",
        "        return torch.einsum('bnsd,bndd->bnsd', q, kv)\n",
        "        # result -> [batch_size, n_heads, seq_len, d_head]\n",
        "\n",
        "    def linear_attention(self, q, k, v):\n",
        "        # stolen from \n",
        "        # https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/kernel_attention.py\n",
        "        # q, k, v -> [batch_size, seq_len, n_heads, d_head]\n",
        "        kv = torch.einsum('bsnx,bsnz->bnxz', k, v)\n",
        "        # kv -> [batch_size, n_heads, d_head, d_head]\n",
        "        # add dropout here\n",
        "        denominator = 1.0 / (torch.einsum('bsnd,bnd->bsn', q, k.sum(axis=1)) + self.eps)\n",
        "        # denominator -> [batch_size, seq_len, n_heads]\n",
        "\n",
        "        output = torch.einsum('bsnx,bnxz,bsn->bsnz', q, kv, denominator).contiguous()\n",
        "        # output -> [batch_size, seq_len, n_heads, d_head]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "        # mask -> [batch_size, 1, seq_len, 1] \n",
        "        q, k, v = torch.chunk(self.w_qkv(x), 3, -1) \n",
        "        # q, k, v -> [batch_size, seq_len, d_model]\n",
        "\n",
        "        q = self.kernel1(self.split_heads(q))\n",
        "        k = self.kernel1(self.split_heads(k))\n",
        "        v = self.zero_out_padded(self.split_heads(v), mask)\n",
        "        # q, k, v -> [batch_size, seq_len, n_heads, d_head]\n",
        "\n",
        "        x = self.linear_attention(q, k, v)\n",
        "        # x -> [batch_size, seq_len, n_heads, d_head]\n",
        "        x = self.join_heads(x)\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20FpV_kB3UPX",
        "outputId": "21cf7cf4-2c55-40c7-d897-2c71ae5acb6e"
      },
      "source": [
        "lens = torch.tensor([5, 8, 2])\n",
        "torch.arange(lens.max(), device=self.dev())[None, :] < lens[:, None]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True,  True,  True,  True,  True, False, False, False],\n",
              "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        [ True,  True, False, False, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHddwETesmJK"
      },
      "source": [
        "batch_size = 2\n",
        "seq_len = 4\n",
        "d_model = 5\n",
        "\n",
        "x = torch.arange(batch_size * seq_len * 3 * d_model).reshape(batch_size, seq_len, 3 * d_model)\n",
        "\n",
        "y = torch.chunk(x, 3, -1)\n",
        "\n",
        "print(x)\n",
        "\n",
        "print(y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8WrImPpvV-z",
        "outputId": "738b117c-cd48-44be-8135-34b7865ed0df"
      },
      "source": [
        "a, b = x.shape[:2]\n",
        "type(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}