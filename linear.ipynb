{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ReyURLrMKo"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJuh2e9praxr"
      },
      "source": [
        "class MHA(nn.Module):\n",
        "    \"\"\"Heart of https://arxiv.org/pdf/2006.16236.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout, kernel, denom_eps, bias):\n",
        "        super(MHA, self).__init__()\n",
        "        assert d_model % n_heads == 0, 'd_model must be a multiple of n_heads'\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = self.d_model // self.n_heads\n",
        "        self.denom_eps = denom_eps\n",
        "\n",
        "        if kernel == 'relu':\n",
        "            self.kernel = self.relu_kernel\n",
        "        elif kernel == 'elu':\n",
        "            self.kernel = self.elu_kernel\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"The only options for 'kernel' are 'relu and 'elu'.\")\n",
        "\n",
        "        self.w_qkv = nn.Linear(self.d_model, 3 * self.d_model, bias=bias)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def apply_mask(self, x, mask):\n",
        "        # x -> [batch_size, seq_len, _]\n",
        "        # mask -> [batch_size, seq_len, 1] or None\n",
        "        if not mask is None:\n",
        "            #x.masked_fill_(~mask, 0)\n",
        "            x = x.masked_fill(~mask, 0)\n",
        "        return x\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len = x.shape[:2]\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "        x = x.view(batch_size, seq_len, self.n_heads, self.d_head)\n",
        "        # x -> [batch_size, seq_len, n_heads, d_head]\n",
        "        return x\n",
        "\n",
        "    def join_heads(self, x):\n",
        "        batch_size, seq_len = x.shape[:2]\n",
        "        # x -> [batch_size, seq_len, n_heads, d_head]\n",
        "        x = x.view(batch_size, seq_len, self.d_model).contiguous()\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "        return x\n",
        "\n",
        "    def elu_kernel(self, x):\n",
        "        return F.elu(x) + 1\n",
        "\n",
        "    def relu_kernel(self, x):\n",
        "        return F.relu(x)\n",
        "\n",
        "    def linear_attention(self, q, k, v):\n",
        "        # stolen from \n",
        "        # https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/kernel_attention.py\n",
        "        # q, k, v -> [batch_size, seq_len, n_heads, d_head]\n",
        "        kv = torch.einsum('bsnx,bsnz->bnxz', k, v)\n",
        "        # kv -> [batch_size, n_heads, d_head, d_head]\n",
        "        # add dropout here\n",
        "        denominator = 1.0 / (torch.einsum('bsnd,bnd->bsn', q, k.sum(axis=1)) + self.denom_eps)\n",
        "        # denominator -> [batch_size, seq_len, n_heads]\n",
        "\n",
        "        output = torch.einsum('bsnx,bnxz,bsn->bsnz', q, kv, denominator).contiguous()\n",
        "        # output -> [batch_size, seq_len, n_heads, d_head]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "        # mask -> [batch_size, seq_len, 1] or None\n",
        "        q, k, v = torch.chunk(self.w_qkv(x), 3, -1) \n",
        "        # q, k, v -> [batch_size, seq_len, d_model]\n",
        "\n",
        "        q = self.kernel(self.split_heads(q))\n",
        "        k = self.kernel(self.split_heads(k))\n",
        "        #v = self.apply_mask(self.split_heads(v), mask)\n",
        "        v = self.split_heads(self.apply_mask(v, mask))\n",
        "        # q, k, v -> [batch_size, seq_len, n_heads, d_head]\n",
        "\n",
        "        x = self.linear_attention(q, k, v)\n",
        "        # x -> [batch_size, seq_len, n_heads, d_head]\n",
        "        x = self.join_heads(x)\n",
        "        x = self.dropout(self.w_o(x))\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_ratio, dropout, bias):\n",
        "        super(FFN, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(d_model, ffn_ratio * d_model, bias=bias),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ffn_ratio * d_model, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "        x = self.layers(x)\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "        return x\n",
        "\n",
        "\n",
        "class MHA_block(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Pre-LN Architecture as suggested here:\n",
        "    https://arxiv.org/pdf/2002.04745.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout, kernel, ffn_ratio, \n",
        "                 ln_eps, denom_eps, bias, rezero):\n",
        "\n",
        "        super(MHA_block, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model, eps=ln_eps)\n",
        "        self.ln2 = nn.LayerNorm(d_model, eps=ln_eps)\n",
        "\n",
        "        self.mha = MHA(d_model, n_heads, dropout, kernel, denom_eps, bias)  \n",
        "        self.ffn = FFN(d_model, ffn_ratio, dropout, bias)\n",
        "\n",
        "        # ReZero is All You Need\n",
        "        # https://arxiv.org/pdf/2003.04887.pdf\n",
        "        # https://github.com/majumderb/rezero\n",
        "        if rezero:\n",
        "            self.alpha = nn.Parameter(torch.Tensor([0]))\n",
        "        else:\n",
        "            self.register_buffer('alpha', torch.Tensor([1]))\n",
        "\n",
        "        # Trick to get model device. Stolen from:\n",
        "        # https://stackoverflow.com/questions/58926054/how-to-get-the-device-type-of-a-pytorch-module-conveniently\n",
        "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def dev(self):\n",
        "        return self.dummy_param.device\n",
        "\n",
        "    def get_mask(self, l, seq_len):\n",
        "        # l -> [batch_size]\n",
        "        mask = torch.arange(seq_len, device=self.dev())[None, :] < l[:, None]\n",
        "        # mask -> [batch_size, seq_len]\n",
        "        mask = mask[..., None]\n",
        "        #mask = torch.logical_not(mask[..., None, None])\n",
        "        # mask -> [batch_size, seq_len, 1\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        # x -> [batch_size, seq_len, d_model]\n",
        "        fx = self.alpha * self.mha(self.ln1(x), mask)\n",
        "        x = x + fx\n",
        "\n",
        "        fx = self.alpha * self.ffn(self.ln2(x))\n",
        "        x = x + fx\n",
        "\n",
        "        return x\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23SWmuJEvYw_"
      },
      "source": [
        "d_model = 16\n",
        "n_heads = 2\n",
        "dropout = 0.2\n",
        "kernel = 'relu'\n",
        "ffn_ratio = 4\n",
        "ln_eps = 1e-6\n",
        "denom_eps = 1e-6\n",
        "bias = False\n",
        "rezero = True\n",
        "\n",
        "seq_len = 3\n",
        "batch_size = 4\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V6l6TJ1P8Sf"
      },
      "source": [
        "model = MHA_block(\n",
        "    d_model = d_model,\n",
        "    n_heads = n_heads, \n",
        "    dropout = dropout, \n",
        "    kernel = kernel, \n",
        "    ffn_ratio = ffn_ratio, \n",
        "    ln_eps = ln_eps, \n",
        "    denom_eps = denom_eps, \n",
        "    bias = bias, \n",
        "    rezero = rezero)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVTWJAfavvIR",
        "outputId": "7d8650cd-58ea-4eb6-b26b-1d9510d740d2"
      },
      "source": [
        "batch = torch.rand((batch_size, seq_len, d_model))\n",
        "print(batch.shape)\n",
        "lens = torch.tensor([1, 3, 2, 1])\n",
        "mask = model.get_mask(lens, seq_len)\n",
        "print(mask.shape)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 16])\n",
            "torch.Size([4, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krWcrKzhwqgi",
        "outputId": "e82ccb5d-4138-4f9c-8fa1-443838567282"
      },
      "source": [
        "output = model(batch, mask)\n",
        "print(output.shape)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxK8CjO-wCdL",
        "outputId": "cfa8f968-e33a-4fc8-f5ec-ec99713b3404"
      },
      "source": [
        "masked_batch = model.apply_mask(batch, mask)\n",
        "print(masked_batch)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.9638, 0.2398, 0.0241, 0.4604, 0.7244, 0.7315, 0.2613, 0.4878,\n",
            "          0.9803, 0.0258, 0.0206, 0.0219, 0.2968, 0.4857, 0.6981, 0.2027],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[0.7327, 0.2265, 0.6460, 0.1862, 0.9855, 0.8580, 0.3619, 0.7935,\n",
            "          0.0060, 0.6024, 0.0805, 0.4093, 0.0561, 0.8490, 0.9586, 0.4716],\n",
            "         [0.7183, 0.1173, 0.7593, 0.5132, 0.6751, 0.1955, 0.3137, 0.3406,\n",
            "          0.7958, 0.2284, 0.6372, 0.4300, 0.2027, 0.9165, 0.8389, 0.8188],\n",
            "         [0.9290, 0.8413, 0.5069, 0.0595, 0.0059, 0.0356, 0.4498, 0.0824,\n",
            "          0.1026, 0.3432, 0.1345, 0.4369, 0.1351, 0.6864, 0.0215, 0.5408]],\n",
            "\n",
            "        [[0.4006, 0.1905, 0.7883, 0.8001, 0.1851, 0.8474, 0.8908, 0.5399,\n",
            "          0.1389, 0.4567, 0.3571, 0.0485, 0.0918, 0.9676, 0.3022, 0.7726],\n",
            "         [0.4523, 0.3758, 0.3709, 0.4534, 0.7516, 0.3824, 0.9377, 0.8032,\n",
            "          0.6450, 0.4015, 0.4323, 0.4816, 0.2166, 0.9133, 0.0845, 0.3430],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[0.0305, 0.0416, 0.0616, 0.3577, 0.5426, 0.6305, 0.5484, 0.2389,\n",
            "          0.0083, 0.8963, 0.5928, 0.9142, 0.3804, 0.1459, 0.2052, 0.6814],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20FpV_kB3UPX",
        "outputId": "21cf7cf4-2c55-40c7-d897-2c71ae5acb6e"
      },
      "source": [
        "lens = torch.tensor([5, 8, 2])\n",
        "torch.arange(lens.max(), device=self.dev())[None, :] < lens[:, None]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True,  True,  True,  True,  True, False, False, False],\n",
              "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        [ True,  True, False, False, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHddwETesmJK"
      },
      "source": [
        "batch_size = 2\n",
        "seq_len = 4\n",
        "d_model = 5\n",
        "\n",
        "x = torch.arange(batch_size * seq_len * 3 * d_model).reshape(batch_size, seq_len, 3 * d_model)\n",
        "\n",
        "y = torch.chunk(x, 3, -1)\n",
        "\n",
        "print(x)\n",
        "\n",
        "print(y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8WrImPpvV-z",
        "outputId": "738b117c-cd48-44be-8135-34b7865ed0df"
      },
      "source": [
        "a, b = x.shape[:2]\n",
        "type(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}